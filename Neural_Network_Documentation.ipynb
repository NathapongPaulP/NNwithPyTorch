{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f44e3deb",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Neural Networks â€” Complete Documentation for NN with PyTorch Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21adcc35",
   "metadata": {},
   "source": [
    "## 1. What is a Neural Network?\n",
    "\n",
    "A **Neural Network (NN)** is a computational system inspired by the human brain. Its goal is to learn patterns from data by adjusting parameters called **weights**.\n",
    "\n",
    "It performs a mapping:\n",
    "\n",
    "\\[\n",
    "\text{Input} \n",
    "ightarrow f(\text{weights}) \n",
    "ightarrow \text{Output}\n",
    "\\]\n",
    "\n",
    "NNs are powerful because they learn **non-linear, high-dimensional** relationships without manual feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b493611c",
   "metadata": {},
   "source": [
    "## 2. Why Do We Use Neural Networks?\n",
    "\n",
    "Neural networks can automatically learn:\n",
    "\n",
    "- Complex relationships  \n",
    "- Non-linear decision boundaries  \n",
    "- High-dimensional data (images, audio, text)  \n",
    "- Features that would be impossible to design manually  \n",
    "\n",
    "This makes them the backbone of modern AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42a96fc",
   "metadata": {},
   "source": [
    "## 3. Building Blocks of a Neural Network\n",
    "\n",
    "### 3.1 Neuron (Perceptron)\n",
    "\n",
    "A single neuron computes:\n",
    "\n",
    "\\[\n",
    "z = w_1x_1 + w_2x_2 + ... + b\n",
    "\\]\n",
    "\n",
    "Then applies an activation:\n",
    "\n",
    "\\[\n",
    "a = \\sigma(z)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(z\\) = weighted sum  \n",
    "- \\(b\\) = bias  \n",
    "- \\(\\sigma\\) = activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a31629d",
   "metadata": {},
   "source": [
    "### 3.2 Layers\n",
    "\n",
    "A neural network stacks neurons into **layers**:\n",
    "\n",
    "- **Input Layer** â€” receives data  \n",
    "- **Hidden Layers** â€” extract features  \n",
    "- **Output Layer** â€” final prediction  \n",
    "\n",
    "Common layer types:\n",
    "- `nn.Linear` (Fully Connected)  \n",
    "- `nn.Conv2d` (Convolution)  \n",
    "- `nn.LSTM` / `nn.GRU`  \n",
    "- Normalization layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29c033",
   "metadata": {},
   "source": [
    "### 3.3 Activation Functions\n",
    "\n",
    "Activation functions introduce **non-linearity**.\n",
    "\n",
    "| Activation | Description | Usage |\n",
    "|-----------|-------------|--------|\n",
    "| **ReLU** | `max(0, x)` | Most common for hidden layers |\n",
    "| **Sigmoid** | Output 0â€“1 | Binary classification |\n",
    "| **Tanh** | Output -1â€“1 | Older RNNs |\n",
    "| **Softmax** | Probabilities | Final layer for multi-class |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba03fc",
   "metadata": {},
   "source": [
    "## 4. How Neural Networks Learn\n",
    "\n",
    "Learning happens in **4 steps**:\n",
    "\n",
    "### 4.1 Forward Pass\n",
    "Data moves through the model â†’ prediction:\n",
    "\\[\n",
    "\\hat{y}\n",
    "\\]\n",
    "\n",
    "### 4.2 Compute Loss\n",
    "Loss measures how wrong the prediction was.\n",
    "\n",
    "### 4.3 Backpropagation\n",
    "Compute gradients of the loss w.r.t. weights:\n",
    "\\[\n",
    "\f\n",
    "rac{\\partial L}{\\partial w}\n",
    "\\]\n",
    "\n",
    "### 4.4 Optimizer Updates Weights\n",
    "Using optimizers like **Adam**:\n",
    "\\[\n",
    "w = w - \\eta \\cdot \f\n",
    "rac{\\partial L}{\\partial w}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad90f17",
   "metadata": {},
   "source": [
    "## 5. Loss Functions\n",
    "\n",
    "### Cross Entropy Loss (Classification)\n",
    "\\[\n",
    "CE = -\\sum y\\log(\\hat{y})\n",
    "\\]\n",
    "\n",
    "### MSE (Regression)\n",
    "\\[\n",
    "MSE = \f\n",
    "rac{1}{n}\\sum (y-\\hat{y})^2\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a720a",
   "metadata": {},
   "source": [
    "## 6. Optimizers\n",
    "\n",
    "| Optimizer | Notes |\n",
    "|----------|-------|\n",
    "| **SGD** | Simple, but slow |\n",
    "| **Adam** | Most common; adaptive |\n",
    "| **RMSprop** | Good for RNNs |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb7511",
   "metadata": {},
   "source": [
    "## 7. Training Workflow in PyTorch\n",
    "\n",
    "The typical workflow:\n",
    "\n",
    "1. **Prepare dataset**  \n",
    "   - Custom dataset or torchvision datasets  \n",
    "   - DataLoader for batching  \n",
    "2. **Build model (nn.Module)**  \n",
    "3. **Define loss function**  \n",
    "4. **Define optimizer**  \n",
    "5. **Training loop**  \n",
    "   - Forward â†’ Loss â†’ Backward â†’ Update  \n",
    "6. **Evaluation loop**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2131eed",
   "metadata": {},
   "source": [
    "## 8. Neural Network Architectures\n",
    "\n",
    "### 8.1 MLP (Fully Connected NN)\n",
    "- Used for small, simple datasets  \n",
    "- Structure: Input â†’ Dense â†’ Activation â†’ Dense\n",
    "\n",
    "### 8.2 CNN (Convolutional Neural Network)\n",
    "Used for images:  \n",
    "- Uses kernels/filters  \n",
    "- Learns spatial patterns\n",
    "\n",
    "### 8.3 RNN / LSTM\n",
    "Used for sequences (text, time-series).  \n",
    "Mostly replaced by Transformers.\n",
    "\n",
    "### 8.4 Transformers\n",
    "Current state of the art:  \n",
    "- Use attention  \n",
    "- Highly parallel  \n",
    "- Dominant in text, vision, speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b51b965",
   "metadata": {},
   "source": [
    "## 9. Bias-Variance\n",
    "\n",
    "### Bias  \n",
    "Error due to oversimplified model (**underfitting**)\n",
    "\n",
    "### Variance  \n",
    "Error due to overly complex model (**overfitting**)\n",
    "\n",
    "### Goal  \n",
    "Balance both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03204d2",
   "metadata": {},
   "source": [
    "## 10. Evaluation Metrics\n",
    "\n",
    "For classification:\n",
    "\n",
    "| Metric | Meaning |\n",
    "|--------|---------|\n",
    "| **Accuracy** | % correct |\n",
    "| **Precision** | Quality of positive predictions |\n",
    "| **Recall** | Ability to find positives |\n",
    "| **F1-score** | Precision + recall balance |\n",
    "| **Confusion Matrix** | Per-class performance |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e6137",
   "metadata": {},
   "source": [
    "## 11. Common Training Problems\n",
    "\n",
    "### Overfitting  \n",
    "Model memorizes training data.  \n",
    "**Fix:** dropout, regularization, augmentation.\n",
    "\n",
    "### Underfitting  \n",
    "Model too simple.  \n",
    "**Fix:** more layers, train longer.\n",
    "\n",
    "### Vanishing Gradients  \n",
    "Gradients become too small.  \n",
    "**Fix:** ReLU, batchnorm, better initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d643090b",
   "metadata": {},
   "source": [
    "## 12. Project Example: SimpleNN (Your MNIST Model)\n",
    "\n",
    "Architecture:\n",
    "\n",
    "- Flatten 28Ã—28 â†’ 784  \n",
    "- Linear(784 â†’ 256)  \n",
    "- ReLU  \n",
    "- Linear(256 â†’ 10)  \n",
    "- Softmax (inside `CrossEntropyLoss`)  \n",
    "\n",
    "Training config:\n",
    "- Loss: `nn.CrossEntropyLoss`  \n",
    "- Optimizer: `Adam`  \n",
    "- Dataset: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ce0b68",
   "metadata": {},
   "source": [
    "## 13. Real-World Applications\n",
    "\n",
    "Neural networks power:\n",
    "\n",
    "- Face recognition  \n",
    "- ChatGPT  \n",
    "- Image classification  \n",
    "- Recommendation systems  \n",
    "- Speech-to-text  \n",
    "- Medical imaging  \n",
    "\n",
    "NNs are foundational to modern AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53e14bb",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "A neural network is a layered function approximator that learns from data using backpropagation and gradient descent.  \n",
    "It is capable of learning complex relationships and forms the core of modern deep learning."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
